{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "projet_IAS_partie2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Eliascc5/English_proficiency_prediction_NLP/blob/main/projet_IAS_partie2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JwVuMzevwDcu",
        "outputId": "e3ff3d79-54ca-41ec-deaf-e78b06611054"
      },
      "source": [
        "#As in the first stage -> we load our dataset from Drive \n",
        "#Dataset: NICT_JLE_4.1\n",
        "#Reference: https://alaginrc.nict.go.jp/nict_jle/index_E.html\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")\n",
        "\n",
        "#----------------------------------\n"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qbJeRExLYyq"
      },
      "source": [
        "import os\n",
        "\n",
        "path_test= r'/content/gdrive/MyDrive/NICT_JLE_4.1/Output/'\n",
        "os.chdir(path_test)\n",
        "\n",
        "vocabulary=[]\n",
        "y_output=[]\n",
        "\n",
        "for file in os.listdir():\n",
        "  if file.endswith(\".txt\"):\n",
        "\n",
        "    file_path = f\"{path_test}/{file}\"\n",
        "\n",
        "    with open(file_path, mode='r',encoding=\"utf8\",errors='ignore') as f:\n",
        "\n",
        "      score = f.readline()\n",
        "      content = f.read()\n",
        "\n",
        "      vocabulary.append(content)\n",
        "\n",
        "      y_output.append(int(score))\n",
        "\n",
        "      f.close\n",
        "\n",
        "######################################"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0D61dNIM4P7L"
      },
      "source": [
        "**NOTE:** We create a general vocabulary from the transcription of all the candidates so that it is as general as possible. \n",
        "\n",
        "Basically the \"vocabulary\" list contains in each element the transcription of each candidate.  In other words -> len (vocabulary) = 1281 \n",
        "\n",
        "Reference: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-8oEcYlOBZV"
      },
      "source": [
        "#TODO: il faut nettoyer \n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Convert a collection of text documents to a matrix of token counts\n",
        "vectorizer = CountVectorizer() \n",
        "# tokenize and build vocab\n",
        "vectorizer.fit(vocabulary)  #(Learn a vocabulary dictionary of all tokens in the raw documents.)\n",
        "# summarize\n",
        "#print(vectorizer.vocabulary_)\n",
        "\n",
        "#print(vectorizer.get_feature_names())\n",
        "\n",
        "# encode document\n",
        "vector = vectorizer.transform(vocabulary)\n",
        "# summarize encoded vector\n",
        "\n",
        "#print(vector.shape)\n",
        "#print(type(vector))\n",
        "#print(vector.toarray())"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1Rji3mKPShl"
      },
      "source": [
        "**Here we start with the Neural Network**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIX8SAgBPFzT"
      },
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "from keras.utils import np_utils\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "#######################################\n",
        "n = len(vocabulary)   #Number of candidates 1281\n",
        "m = 15157  #Number of words without repetition of our vocabulary \n",
        "\n",
        "array=vector.toarray()                      #INPUT\n",
        "\n",
        "array_output = np.array(y_output)           #OUTPUT\n",
        "\n",
        "array_output = np_utils.to_categorical(array_output) \n",
        "\n",
        "array_output = array_output[:,1:10]\n",
        "\n",
        "\n",
        "########################################\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(200, activation='sigmoid', input_shape=(m,)))\n",
        "model.add(layers.Dense(400, activation='sigmoid'))\n",
        "model.add(layers.Dense(200, activation='sigmoid'))\n",
        "model.add(layers.Dense(9, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "######################################################################\n",
        "\n",
        "#Repartition des donnees\n",
        "randomGenerator = np.random.RandomState(0) #Generateur de nombres aleatoires\n",
        "nbIndividus = len(vocabulary) #Nombre d'individus presents dans la base de donnees\n",
        "randomIndexes = np.arange(nbIndividus)#Creation d'une liste d'index\n",
        "randomGenerator.shuffle(randomIndexes)#Melange de cette liste\n",
        "#Utilisation de cette liste d'index pour repartir les donnees en differents jeux\n",
        "#Apprentissage : 60%\n",
        "x_train_array = array[randomIndexes[0:int(len(vocabulary)*0.6)]]\n",
        "y_train_array = array_output[randomIndexes[0:int(len(vocabulary)*0.6)]]\n",
        "#Validation : 20 %\n",
        "x_val_array = array[randomIndexes[int(len(vocabulary)*0.6):int(len(vocabulary)*0.8)]]\n",
        "y_val = array_output[randomIndexes[int(len(vocabulary)*0.6):int(len(vocabulary)*0.8)]]\n",
        "#Test : 20%\n",
        "x_test_array = array[randomIndexes[int(len(vocabulary)*0.8):]]\n",
        "y_test = array_output[randomIndexes[int(len(vocabulary)*0.8):]]\n",
        "\n",
        "\n",
        "history = model.fit(x_train_array,\n",
        "                    y_train_array,\n",
        "                    epochs=50,\n",
        "                    batch_size=512,\n",
        "                    validation_data=(x_val_array, y_val))\n",
        "\n",
        "#plot the results :\n",
        "history_dict = history.history\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "# \"bo\" is for \"blue dot\"\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "# b is for \"solid blue line\"\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXiykoVy7Wmw"
      },
      "source": [
        "**TODO:** add prediction stage; Network performance tests   **IN PROCESS**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oF1HCF7HFzP"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pr7OgigJBAfH"
      },
      "source": [
        "##IN PROCESS....\n",
        "predictions= model.predict(x_test_array) \n",
        "\n",
        "for i in range(10):\n",
        "  print(\"Prediction: \", np.argmax(predictions[i])+1)  #add +1 to start in 1\n",
        "  print(\"Label of x_test\", y_test[i])\n",
        "  print(\"------------------\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}